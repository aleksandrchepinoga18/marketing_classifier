# src/train_final.py
import pandas as pd
import numpy as np
import json
import os
import sys
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    f1_score, average_precision_score,
    classification_report, roc_auc_score
)
import shap
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å (–Ω–∞ —Å–ª—É—á–∞–π –∑–∞–ø—É—Å–∫–∞ –Ω–∞–ø—Ä—è–º—É—é)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –∏–º–ø–æ—Ä—Ç
from data import load_and_clean_data

def main():
    OUTPUT_DIR = "reports/model"
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # -------------------------------------------------
    # 1. –ó–ê–ì–†–£–ó–ö–ê –ò –û–ß–ò–°–¢–ö–ê
    # -------------------------------------------------
    df = load_and_clean_data("ifood_df.csv")
    X = df.drop(columns=['Response'])
    y = df['Response']

    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, stratify=y_temp, random_state=42)

    print(f"–†–∞–∑–º–µ—Ä—ã: train={len(y_train)}, val={len(y_val)}, test={len(y_test)}")
    print(f"–ë–∞–ª–∞–Ω—Å (Response=1): train={y_train.mean():.3f}, val={y_val.mean():.3f}, test={y_test.mean():.3f}")

    # -------------------------------------------------
    # 2. –ü–ê–†–ê–ú–ï–¢–†–´
    # -------------------------------------------------
    best_params = {
        "objective": "binary",
        "metric": "binary_logloss",
        "verbosity": -1,
        "boosting_type": "gbdt",
        "num_leaves": 31,
        "min_child_samples": 100,
        "feature_fraction": 0.7,
        "bagging_fraction": 0.7,
        "bagging_freq": 5,
        "lambda_l1": 1.0,
        "lambda_l2": 1.0,
        "learning_rate": 0.05,
        "scale_pos_weight": len(y_train[y_train == 0]) / len(y_train[y_train == 1]),
        "max_depth": 5,
        "min_data_in_leaf": 100
    }

    print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –¥–µ–ø–ª–æ—è")
    with open(os.path.join(OUTPUT_DIR, "params.json"), "w") as f:
        json.dump(best_params, f, indent=4)

    # -------------------------------------------------
    # 3. –û–ë–£–ß–ï–ù–ò–ï
    # -------------------------------------------------
    train_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

    final_model = lgb.train(
        best_params,
        train_data,
        valid_sets=[val_data],
        valid_names=['valid'],
        num_boost_round=2000,
        callbacks=[
            lgb.early_stopping(stopping_rounds=100, verbose=True),
            lgb.log_evaluation(50)
        ]
    )

    final_model.save_model(os.path.join(OUTPUT_DIR, "model.txt"))

    # -------------------------------------------------
    # 4. –ü–û–î–ë–û–† –ü–û–†–û–ì–ê –ù–ê VAL
    # -------------------------------------------------
    y_val_proba = final_model.predict(X_val)
    thresholds = np.arange(0.01, 1.0, 0.01)
    f1_scores = [f1_score(y_val, (y_val_proba >= th).astype(int)) for th in thresholds]
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx]

    print(f"\nüî¢ –õ—É—á—à–∏–π –ø–æ—Ä–æ–≥ (–ø–æ F1 –Ω–∞ val): {best_threshold:.4f}")

    # -------------------------------------------------
    # 5. –ú–ï–¢–†–ò–ö–ò –ù–ê –í–°–ï–• –í–´–ë–û–†–ö–ê–• (–≤–∫–ª—é—á–∞—è ROC-AUC –Ω–∞ val!)
    # -------------------------------------------------
    y_train_proba = final_model.predict(X_train)
    y_val_proba = final_model.predict(X_val)
    y_test_proba = final_model.predict(X_test)

    y_train_pred = (y_train_proba >= best_threshold).astype(int)
    y_val_pred = (y_val_proba >= best_threshold).astype(int)
    y_test_pred = (y_test_proba >= best_threshold).astype(int)

    # F1
    train_f1 = f1_score(y_train, y_train_pred)
    val_f1 = f1_score(y_val, y_val_pred)
    test_f1 = f1_score(y_test, y_test_pred)

    # ROC-AUC (–≥–ª–∞–≤–Ω–æ–µ ‚Äî —Ç–µ–ø–µ—Ä—å –µ—Å—Ç—å –Ω–∞ val!)
    train_roc_auc = roc_auc_score(y_train, y_train_proba)
    val_roc_auc = roc_auc_score(y_val, y_val_proba)
    test_roc_auc = roc_auc_score(y_test, y_test_proba)

    # PR-AUC
    test_pr_auc = average_precision_score(y_test, y_test_proba)

    # –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
    overfitting_f1 = (train_f1 - val_f1) > 0.05
    overfitting_roc = (train_roc_auc - val_roc_auc) > 0.02
    overfitting_detected = overfitting_f1 or overfitting_roc

    metrics = {
        "best_threshold": float(best_threshold),
        "train_f1": float(train_f1),
        "val_f1": float(val_f1),
        "test_f1": float(test_f1),
        "train_roc_auc": float(train_roc_auc),
        "val_roc_auc": float(val_roc_auc),
        "test_roc_auc": float(test_roc_auc),
        "test_pr_auc": float(test_pr_auc),
        "overfitting_detected": bool(overfitting_detected),
        "overfitting_f1": bool(overfitting_f1),
        "overfitting_roc_auc": bool(overfitting_roc),
        "class_balance_test": float(y_test.mean()),
        "best_iteration": int(final_model.best_iteration)
    }

    with open(os.path.join(OUTPUT_DIR, "metrics.json"), "w") as f:
        json.dump(metrics, f, indent=4)

    print("\nüìä –ú–µ—Ç—Ä–∏–∫–∏:")
    for k, v in metrics.items():
        if isinstance(v, float):
            print(f"  {k}: {v:.4f}")
        else:
            print(f"  {k}: {v}")

    # -------------------------------------------------
    # 6. CLASSIFICATION REPORT
    # -------------------------------------------------
    report_str = classification_report(y_test, y_test_pred)
    RUN_TIME = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    report_with_header = f"–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞: {RUN_TIME}\n–ü–æ—Ä–æ–≥: {best_threshold:.4f}\n\n" + report_str

    report_path = os.path.join(OUTPUT_DIR, "classification_report.txt")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report_with_header)

    print("\n" + "="*60)
    print("üìã FULL CLASSIFICATION REPORT (TEST):")
    print("="*60)
    print(report_str)
    print("="*60)
    print(f"\n‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {os.path.abspath(report_path)}")

    # -------------------------------------------------
    # 7. –ê–†–¢–ï–§–ê–ö–¢–´
    # -------------------------------------------------
    importance_df = pd.DataFrame({
        'feature': X_train.columns,
        'importance': final_model.feature_importance()
    }).sort_values('importance', ascending=False)
    importance_df.to_csv(os.path.join(OUTPUT_DIR, "feature_importance.csv"), index=False)

    plt.figure(figsize=(8, 10))
    importance_df.head(20).plot.barh(x='feature', y='importance', legend=False)
    plt.gca().invert_yaxis()
    plt.title("Feature Importance (Final Model)")
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, "feature_importance.png"), dpi=150)
    plt.close()

    explainer = shap.TreeExplainer(final_model)
    shap_values = explainer.shap_values(X_val)
    shap_vals = shap_values[1] if isinstance(shap_values, list) else shap_values

    shap.summary_plot(shap_vals, X_val, show=False)
    plt.savefig(os.path.join(OUTPUT_DIR, "shap_summary.png"), dpi=150, bbox_inches='tight')
    plt.close()

    X_test.sample(5, random_state=42).to_csv(os.path.join(OUTPUT_DIR, "sample_input.csv"), index=False)

    print(f"\nüéâ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∏ –≤—Å–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ '{OUTPUT_DIR}'")

if __name__ == "__main__":
    main()